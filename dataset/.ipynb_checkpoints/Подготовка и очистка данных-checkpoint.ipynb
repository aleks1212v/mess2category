{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "import pymorphy2\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Дата подачи заявки', 'Долгота', 'Широта', 'Адрес',\n",
      "       'Категория, присвоенная модератором', 'Описание заявки',\n",
      "       'Последний исполнитель', 'id исполнителя', 'Статус заявки'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Требую сделать!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('tatarstan_message_правка_2.csv', sep = ';', encoding= 'utf-8')\n",
    "print(data.columns)\n",
    "\n",
    "data = data[['Категория, присвоенная модератором', 'Описание заявки', 'Последний исполнитель']]\n",
    "data.columns = ['categories', 'description', 'worker']\n",
    "data.head(3)\n",
    "data.loc[18685]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Благоустройство территории', 'Содержание и ремонт муниципальных дорог', 'Поликлиники и больницы', 'Бездомные животные', 'Парки и скверы', 'Мобильная связь', 'Организация дорожного движения', 'Незаконные азартные игры', 'Санитарное состояние', 'Нарушение в наружной рекламе', 'Работа спортивных объектов', 'Капитальный ремонт', 'Общественный транспорт', 'Воздух', 'Садоводческие, огороднические и дачные некоммерческие объединения', 'Детские сады', 'Доступная среда', 'Жилищное строительство', 'Почта', 'Нарушение правил торговли', 'Пенсионное обеспечение', 'Вода', nan, 'Содержание и ремонт федеральных и республиканских дорог', 'Меры социальной поддержки, предоставляемые органами социальной защиты', 'Cвалки', 'Работа учреждений культуры', 'Пособия безработным', 'Социальное обслуживание', 'Объекты культурного наследия', 'Сельское хозяйство', 'Цифровое телевидение', 'Тех.средства и услуги по реабилитации инвалидов за счет средств федерального бюджета', 'Опека, попечительство', 'Ошибки в названиях и надписях', 'Диспансеризация населения', 'Незаконная добыча полезных ископаемых']\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "for cat in data.categories:\n",
    "    if cat not in categories:\n",
    "        categories.append(cat)\n",
    "print(categories)\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    def replace_three_or_more(text): #remove dublicate of symbol\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "        return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    def emo_to_text(text):  \n",
    "        emoticon = {'улыбка шутка' : ['\\:\\)', '\\:\\-\\)', '\\:\\=\\)'],\n",
    "                    'грусть разочарование' : ['\\:\\(', '\\:\\-\\(', '\\:\\=\\('],\n",
    "                    'открытая радость' : ['\\=\\)', '\\=\\-\\)'],\n",
    "                    'усмешка хихиканье' : ['\\:>', '\\:\\->', '\\:\\=>'],\n",
    "                    'улыбка' : ['\\:\\]', '\\:\\-\\]', '\\:\\=\\]'],\n",
    "                    'смех' : ['\\:D', '\\:\\-D', '\\:\\=D'],\n",
    "                    'сильный смех' : ['\\:DD', '\\:\\-DD', '\\:\\=DD'],\n",
    "                    'сарказм' : ['\\:\\}', '\\:\\-\\}', '\\:\\=\\}'],\n",
    "                    'подмигивание заигрывание' : ['\\;\\)', '\\;\\-\\)', '\\;\\=\\)'],\n",
    "                    'молчание' : ['\\:Х', '\\:\\-Х', '\\:\\=Х'],\n",
    "                    'удивленное разочарование' : ['8\\(', '8\\-\\(', '8\\=\\('],\n",
    "                    'изумление' : ['B\\-о', 'B\\=о'],\n",
    "                    'радостное удивление' : ['\\%\\)'],\n",
    "                    'зевота' : ['\\|\\-o', '\\|\\=o'],\n",
    "                    'смущение' : ['\\:S', '\\:-S', '\\:\\=S'],\n",
    "                    'равнодушие скука' : ['\\:\\|', '\\:\\-\\|', '\\:\\=\\|'],\n",
    "                    'недоверие сомнение' : ['\\:\\/', '\\:\\-\\/', '\\:\\=\\/'],\n",
    "                    'дразнящее показывание языка' : ['\\:ь', '\\:\\-ь', '\\:\\=ь'],\n",
    "                    'поцелуй' : ['\\:\\*', '\\:\\^\\*', '\\:\\-Ф'],\n",
    "                    'шалость дурачество' : ['\\:0\\)'],\n",
    "                    'кривая ухмылка' : ['\\:7', '\\:\\-7']}\n",
    "\n",
    "        for key, value in emoticon.items():\n",
    "            text = re.sub(r\"|\".join(value), ' ' + key, text)\n",
    "        return text\n",
    "\n",
    "    try:\n",
    "        text = emo_to_text(text)\n",
    "        text = emoji.demojize(text, delimiters=(\"\", \"\")) #преобразует эмодзи-символы в слова\n",
    "        text = text.lower().replace(\"ё\", \"е\")\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
    "        text = re.sub('@[^\\s]+', 'USER', text)\n",
    "        text = re.sub('[^a-zA-Zа-яА-Я]+', ' ', text) #удаляем цифры и спецсимволы\n",
    "        text = replace_three_or_more(text) #удаляет дублирование букв, если число букв более 3\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        return text.strip()\n",
    "    except:\n",
    "        #print(f'Слово {text} не прошло препроцессинг')\n",
    "        return '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description_prep'] = data['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         28\n",
      "1         36\n",
      "2         21\n",
      "3         21\n",
      "4         39\n",
      "        ... \n",
      "29131    536\n",
      "29132      1\n",
      "29133      1\n",
      "29134      1\n",
      "29135      1\n",
      "Name: num_words, Length: 29136, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data['num_words'] = data['description_prep'].apply(str.split, args = (' '))\n",
    "data['num_words'] = data['num_words'].apply(len)\n",
    "print(data['num_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         28\n",
      "1         36\n",
      "2         21\n",
      "3         21\n",
      "4         39\n",
      "        ... \n",
      "23693     22\n",
      "23694     17\n",
      "23695     17\n",
      "23696     41\n",
      "23697    536\n",
      "Name: num_words, Length: 23698, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = data.loc[data['num_words'] > 3] #удалить строку, если меньше 4-х слов\n",
    "data.reset_index(inplace=True)\n",
    "print(data['num_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Благоустройство территории': 10953, 'Содержание и ремонт муниципальных дорог': 6232, 'Поликлиники и больницы': 521, 'Бездомные животные': 684, 'Парки и скверы': 385, 'Мобильная связь': 193, 'Организация дорожного движения': 1684, 'Незаконные азартные игры': 10, 'Санитарное состояние': 90, 'Нарушение в наружной рекламе': 1017, 'Работа спортивных объектов': 26, 'Капитальный ремонт': 127, 'Общественный транспорт': 880, 'Воздух': 139, 'Садоводческие, огороднические и дачные некоммерческие объединения': 13, 'Детские сады': 127, 'Доступная среда': 76, 'Жилищное строительство': 34, 'Почта': 43, 'Нарушение правил торговли': 4, 'Пенсионное обеспечение': 2, 'Вода': 26, nan: 0, 'Содержание и ремонт федеральных и республиканских дорог': 39, 'Меры социальной поддержки, предоставляемые органами социальной защиты': 8, 'Cвалки': 304, 'Работа учреждений культуры': 16, 'Пособия безработным': 2, 'Социальное обслуживание': 3, 'Объекты культурного наследия': 15, 'Сельское хозяйство': 18, 'Цифровое телевидение': 18, 'Тех.средства и услуги по реабилитации инвалидов за счет средств федерального бюджета': 2, 'Опека, попечительство': 1, 'Ошибки в названиях и надписях': 1, 'Диспансеризация населения': 2, 'Незаконная добыча полезных ископаемых': 3}\n"
     ]
    }
   ],
   "source": [
    "cat_num = dict()\n",
    "for cat in categories:\n",
    "    num = np.sum(data['categories'] == cat)\n",
    "    cat_num.update({cat : num})\n",
    "print(cat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         28\n",
      "1         36\n",
      "2         21\n",
      "3         21\n",
      "4         39\n",
      "        ... \n",
      "23665     22\n",
      "23666     17\n",
      "23667     17\n",
      "23668     41\n",
      "23669    536\n",
      "Name: num_words, Length: 23670, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Фильтрация категорий, в которых менее 10 обращений\n",
    "for key, value in cat_num.items():\n",
    "    if value < 10:\n",
    "        data = data.loc[data['categories'] != key]\n",
    "data.reset_index(inplace=True)\n",
    "print(data['num_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Благоустройство территории', 'Содержание и ремонт муниципальных дорог', 'Поликлиники и больницы', 'Бездомные животные', 'Парки и скверы', 'Мобильная связь', 'Организация дорожного движения', 'Незаконные азартные игры', 'Санитарное состояние', 'Нарушение в наружной рекламе', 'Работа спортивных объектов', 'Капитальный ремонт', 'Общественный транспорт', 'Воздух', 'Садоводческие, огороднические и дачные некоммерческие объединения', 'Детские сады', 'Доступная среда', 'Жилищное строительство', 'Почта', 'Вода', 'Содержание и ремонт федеральных и республиканских дорог', 'Cвалки', 'Работа учреждений культуры', 'Объекты культурного наследия', 'Сельское хозяйство', 'Цифровое телевидение']\n",
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' выводятся из файла модели BERT\\ncat_path = \"flask/categories.txt\"\\nwith open(cat_path, \\'w\\') as f:\\n    for n, cat in enumerate(categories):\\n        f.write(str(n) + \\';\\' + cat + \\'\\n\\')'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = []\n",
    "for cat in data.categories:\n",
    "    if cat not in categories:\n",
    "        categories.append(cat)\n",
    "print(categories)\n",
    "print(len(categories))\n",
    "\"\"\" выводятся из файла модели BERT\n",
    "cat_path = \"flask/categories.txt\"\n",
    "with open(cat_path, 'w') as f:\n",
    "    for n, cat in enumerate(categories):\n",
    "        f.write(str(n) + ';' + cat + '\\n')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#извлечение именованных сущностей\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    LOC,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "def delete_NER(words):\n",
    "    nf_words = ' '.join(words)\n",
    "    per_words = []\n",
    "    loc_words = []\n",
    "    doc = Doc(nf_words)\n",
    "\n",
    "    doc.segment(segmenter)\n",
    "\n",
    "    doc.tag_ner(ner_tagger)\n",
    "\n",
    "    for span in doc.spans:\n",
    "        span.normalize(morph_vocab)\n",
    "    for span in doc.spans:\n",
    "        if span.type == 'PER':\n",
    "            span.extract_fact(names_extractor)\n",
    "            per_words.append(span.text)\n",
    "        if span.type == 'LOC':\n",
    "            span.extract_fact(names_extractor)\n",
    "            loc_words.append(span.text)\n",
    "       \n",
    "    for word in per_words:\n",
    "        if word in nf_words:\n",
    "            nf_words = nf_words.replace(word, ' PER ')\n",
    "    for word in loc_words:\n",
    "        if word in nf_words:\n",
    "            nf_words = nf_words.replace(word, ' LOC ')\n",
    "    words = nf_words.split(' ')\n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаление именованных сущностей из наименования исполнителей\n",
    "worker = []\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "for row in data['worker']:\n",
    "    text = str(row)\n",
    "    text = text.split(' ')\n",
    "    text = delete_NER(text)\n",
    "    text = ' '.join(text)\n",
    "    text = text.replace(\"ё\", \"е\")\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я]+', ' ', text) #удаляем цифры и спецсимволы\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    words = text.split(' ')\n",
    "    words = [w.strip() for w in words if (not w in ['PER', 'LOC']) and (len(w) > 1) and w != None and w != '']\n",
    "    w_ = ''\n",
    "    for n, w in enumerate(words):\n",
    "        if w == w_:\n",
    "            words[n] = ''\n",
    "        w_ = w\n",
    "    words = ' '.join(words) #ttext_stem\n",
    "    if words[-3:] == ' по':\n",
    "        words = words[:-3]\n",
    "    if words[-2:] == ' и':\n",
    "        words = words[:-2]\n",
    "    if words != None and words.strip() != '' and words.strip() != 'nan':\n",
    "        worker.append(words)\n",
    "    else:\n",
    "        worker.append('Неизвестный исполнитель')\n",
    "data['worker'] = pd.Series(worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                Исполнительный комитет\n",
      "1                                         Администрация\n",
      "2                          Министерство здравоохранения\n",
      "3                       Муниципальное образование город\n",
      "4      Управление административно технической инспекции\n",
      "                            ...                        \n",
      "77            Приикское территориальное управление МЭПР\n",
      "78    Министерство по делам гражданской обороны чрез...\n",
      "79             Министерство экологии природных ресурсов\n",
      "80             Северное территориальное управление МЭПР\n",
      "81    Палата земельных имущественных отношений Альме...\n",
      "Length: 82, dtype: object\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "wo = []\n",
    "for w in data.worker:\n",
    "    if w not in wo:\n",
    "        wo.append(w)\n",
    "wo = pd.Series(wo)\n",
    "print(wo)\n",
    "print(len(wo))\n",
    "executor_path = \"flask/executors.txt\"\n",
    "with open(executor_path, 'w') as f:\n",
    "    for n, e in enumerate(wo):\n",
    "        f.write(str(n) + ';' + e + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n, row in enumerate(data['worker']):\n",
    "#    if 'заявка решена' in str(row):\n",
    "#        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[14589]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['worker'][14589]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стемминг\n",
    "morph = pymorphy2.MorphAnalyzer() #по умолчанию русский язык\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "\n",
    "ttext_stem = list()\n",
    "for row in data['description_prep']:\n",
    "    words = row.split(' ')\n",
    "    words = delete_NER(words) #удаление именованных сущностей: ФИО и гео-данных\n",
    "    nf_words = list()\n",
    "    for word in words:\n",
    "        try:\n",
    "            #word = word.encode('utf8mb4')\n",
    "            ru_word = re.sub('[^а-яА-Я]+', ' ', word)\n",
    "            en_word = re.sub('[^a-zA-Z]+', ' ', word)\n",
    "            if ru_word == word and word != '': # кириллица\n",
    "                parse_word = morph.parse(word)[0]\n",
    "                nf_word = parse_word.normal_form\n",
    "                nf_words.append(nf_word.strip())\n",
    "            elif en_word == word and word != '': # латиница\n",
    "                nf_word = stemmer.stem(word)\n",
    "                nf_words.append(nf_word.strip())\n",
    "            else:\n",
    "                pass #смешанные слова из кириллицы и латиницы не анализируются\n",
    "        except Exception as e:\n",
    "                print(f'Слово не преобразовано {e}') \n",
    "                continue\n",
    "    nf_words = [w for w in nf_words if not w in stops]\n",
    "    \n",
    "    nf_words = ' '.join(nf_words) #ttext_stem\n",
    "    nf_words = nf_words.lower().replace(\"ё\", \"е\")\n",
    "    ttext_stem.append(nf_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        трасса нижнекамск чистополь лесополоса пгт кам...\n",
      "1        двор дом ленинградский отсутствовать парковочн...\n",
      "2        добрый день осуществляться уборка снег центр г...\n",
      "3        добрый день осуществляться вывоз мусор террито...\n",
      "4        дать участок проезжий часть ул халитовый дом у...\n",
      "                               ...                        \n",
      "23665    устранить прицеп магазин цифровик парковка тц ...\n",
      "23666    производиться уборка детский площадка дом корп...\n",
      "23667    свалка отход мусор гаражный кооператив автомоб...\n",
      "23668    здравствовать каждый год пора начать привозить...\n",
      "23669    сентябрь программа развитие садоводческий движ...\n",
      "Name: description_stem, Length: 23670, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if ttext_stem != None:\n",
    "    ttext_stem = pd.Series(ttext_stem) \n",
    "    data['description_stem'] = ttext_stem  \n",
    "    #data['description_stem'] = '\"\"' + data['description_stem'] + '\"\"'\n",
    "    print(data['description_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[['description_stem','categories']]\n",
    "data.to_csv(\"tatarstan_message_2.csv\", sep = ',',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
